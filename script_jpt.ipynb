{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5846d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f79e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_reduced_quant = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mdf_reduced_quant.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_reduced_quant = pd.read_csv(\"df_reduced_quant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7355f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_reduced_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded01ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    21\n",
       "int64      18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e1447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int32      15\n",
       "float32    11\n",
       "float16     8\n",
       "float64     2\n",
       "int8        2\n",
       "int64       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduzir_tipos(df):\n",
    "    df_otimizado = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            col_min = col_data.min()\n",
    "            col_max = col_data.max()\n",
    "\n",
    "            # transforma em booleano se tiver só 0 e 1\n",
    "            if set(col_data.unique()).issubset({0, 1}):\n",
    "                df_otimizado[col] = col_data.astype(bool)\n",
    "                continue\n",
    "\n",
    "            # inteirso\n",
    "            if pd.api.types.is_integer_dtype(col_data):\n",
    "                if np.iinfo(np.int8).min <= col_min and col_max <= np.iinfo(np.int8).max:\n",
    "                    df_otimizado[col] = col_data.astype(np.int8)\n",
    "                elif np.iinfo(np.int16).min <= col_min and col_max <= np.iinfo(np.int16).max:\n",
    "                    df_otimizado[col] = col_data.astype(np.int16)\n",
    "                elif np.iinfo(np.int32).min <= col_min and col_max <= np.iinfo(np.int32).max:\n",
    "                    df_otimizado[col] = col_data.astype(np.int32)\n",
    "                else:\n",
    "                    df_otimizado[col] = col_data.astype(np.int64)\n",
    "\n",
    "            #floats\n",
    "            elif pd.api.types.is_float_dtype(col_data):\n",
    "                if np.finfo(np.float16).min <= col_min and col_max <= np.finfo(np.float16).max:\n",
    "                    df_otimizado[col] = col_data.astype(np.float16)\n",
    "                elif np.finfo(np.float32).min <= col_min and col_max <= np.finfo(np.float32).max:\n",
    "                    df_otimizado[col] = col_data.astype(np.float32)\n",
    "                else:\n",
    "                    df_otimizado[col] = col_data.astype(np.float64)\n",
    "\n",
    "    return df_otimizado\n",
    "df_teste = reduzir_tipos(df)\n",
    "df_teste.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cf4897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.to_parquet(\"df_teste.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53caecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int32      15\n",
       "float32    11\n",
       "float16     8\n",
       "float64     2\n",
       "int8        2\n",
       "int64       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste1 = pd.read_parquet(\"df.parquet\")\n",
    "df_teste1.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46ca45cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del  df_teste1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc36f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9430663 entries, 0 to 9430662\n",
      "Data columns (total 39 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   fwd_iat_mean       float32\n",
      " 1   bwd_pkt_len_mean   float16\n",
      " 2   fwd_iat_tot        float32\n",
      " 3   pkt_len_var        float32\n",
      " 4   init_fwd_win_byts  int32  \n",
      " 5   subflow_bwd_byts   int32  \n",
      " 6   pkt_len_mean       float16\n",
      " 7   pkt_size_avg       float16\n",
      " 8   bwd_header_len     int32  \n",
      " 9   totlen_bwd_pkts    float32\n",
      " 10  fwd_act_data_pkts  int32  \n",
      " 11  flow_byts/s        float64\n",
      " 12  flow_iat_mean      float32\n",
      " 13  fwd_pkts/s         float32\n",
      " 14  tot_bwd_pkts       int32  \n",
      " 15  flow_pkts/s        float64\n",
      " 16  bwd_pkt_len_std    float16\n",
      " 17  pkt_len_std        float16\n",
      " 18  bwd_seg_size_avg   float16\n",
      " 19  fwd_iat_min        float32\n",
      " 20  pkt_len_max        int32  \n",
      " 21  flow_iat_max       float32\n",
      " 22  tot_fwd_pkts       int32  \n",
      " 23  bwd_pkt_len_max    int32  \n",
      " 24  bwd_pkts/s         float32\n",
      " 25  flow_iat_min       float32\n",
      " 26  fwd_header_len     int32  \n",
      " 27  fwd_seg_size_min   int8   \n",
      " 28  totlen_fwd_pkts    int32  \n",
      " 29  fwd_pkt_len_mean   float16\n",
      " 30  flow_duration      int64  \n",
      " 31  subflow_bwd_pkts   int32  \n",
      " 32  fwd_iat_max        float32\n",
      " 33  fwd_seg_size_avg   float16\n",
      " 34  init_bwd_win_byts  int32  \n",
      " 35  fwd_pkt_len_max    int32  \n",
      " 36  subflow_fwd_pkts   int32  \n",
      " 37  subflow_fwd_byts   int32  \n",
      " 38  label              int8   \n",
      "dtypes: float16(8), float32(11), float64(2), int32(15), int64(1), int8(2)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "df_teste1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b687a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_otimizado.drop('label', axis=1)\n",
    "y = df_otimizado['label']\n",
    "\n",
    "x_clean = x.replace([np.inf, -np.inf], np.nan)\n",
    "x_clean = x_clean.fillna(x_clean.mean())\n",
    "\n",
    "x1 = x_clean.values\n",
    "y1 = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd09ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_otimizado, x, y, x_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa20e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=30, n_jobs=-1, random_state=42)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "test_scores_RF_mdi, precision_scores_RF_mdi, recall_scores_RF_mdi, f1_scores_RF_mdi = [], [], [], []\n",
    "feature_importances_list = []\n",
    "\n",
    "rf_base = RandomForestClassifier(max_features='sqrt', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441c94bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x1, y1, test_size=0.2, stratify=y1, random_state=42)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "feature_importances_list.append(rf.feature_importances_)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "test_scores_RF_mdi.append(accuracy_score(Y_test, y_pred))\n",
    "precision_scores_RF_mdi.append(precision_score(Y_test, y_pred, average='weighted'))\n",
    "recall_scores_RF_mdi.append(recall_score(Y_test, y_pred, average='weighted'))\n",
    "f1_scores_RF_mdi.append(f1_score(Y_test, y_pred, average='weighted'))\n",
    "\n",
    "del X_train, X_test, Y_train, Y_test, rf, y_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c9f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução de features\n",
    "importances_mean = np.mean(feature_importances_list, axis=0)\n",
    "importances_series = pd.Series(importances_mean, index=df_otimizado.drop('label', axis=1).columns)\n",
    "selected_features = importances_series[importances_series >= importances_series.median()].index.tolist()\n",
    "df_reduced = df_otimizado[selected_features + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988a62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mdi = pd.DataFrame({\n",
    "    \"Modelo\": [\"RandomForest\"],\n",
    "    \"Acurácia\": [np.mean(test_scores_RF_mdi)],\n",
    "    \"Precisão\": [np.mean(precision_scores_RF_mdi)],\n",
    "    \"Recall\": [np.mean(recall_scores_RF_mdi)],\n",
    "    \"F1 Score\": [np.mean(f1_scores_RF_mdi)]\n",
    "})\n",
    "\n",
    "metrics_mdi.to_csv(\"metrics_mdi.csv\", index=False)\n",
    "df_reduced.to_csv(\"df_reduced_quant.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4350e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.columns.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16452d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikae\\AppData\\Local\\Temp\\ipykernel_3068\\286496714.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df.groupby('label').apply(lambda x: x.sample(n=1000, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_sampled = df.groupby('label').apply(lambda x: x.sample(n=1000, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8463c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.to_csv(\"df_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6e9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1000\n",
       "1    1000\n",
       "2    1000\n",
       "3    1000\n",
       "4    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd86fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c9096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f287bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae4fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea264a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = pd.read_parquet(\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3703498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\pandas\\core\\nanops.py:1496: RuntimeWarning: overflow encountered in cast\n",
      "  return count.astype(dtype, copy=False)\n",
      "c:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:52: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    " # KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x = df_reduced.drop('label', axis=1)\n",
    "y = df_reduced['label']\n",
    "\n",
    "x_clean = x.replace([np.inf, -np.inf], np.nan)\n",
    "x_clean = x_clean.fillna(x_clean.mean())\n",
    "\n",
    "x1 = x_clean.values\n",
    "y1 = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a480eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3717"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_reduced, x, y, x_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112db851",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.44 TiB for an array with shape (6191212, 142914) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m valid_classes = {\u001b[38;5;28mcls\u001b[39m: \u001b[32m5000\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m counts.get(\u001b[38;5;28mcls\u001b[39m, \u001b[32m0\u001b[39m) >= \u001b[32m5000\u001b[39m}\n\u001b[32m      9\u001b[39m nm = NearMiss(sampling_strategy=valid_classes, version=\u001b[32m2\u001b[39m, n_neighbors=\u001b[32m3\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_resampled, Y_resampled = \u001b[43mnm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m hash_train = hash_pandas_object(pd.DataFrame(X_train)).values\n\u001b[32m     13\u001b[39m hash_resampled = hash_pandas_object(pd.DataFrame(X_resampled)).values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\imblearn\\base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\imblearn\\base.py:105\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     99\u001b[39m X, y, binarize_y = \u001b[38;5;28mself\u001b[39m._check_X_y(X, y)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m y_ = (\n\u001b[32m    108\u001b[39m     label_binarize(output[\u001b[32m1\u001b[39m], classes=np.unique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m X_, y_ = arrays_transformer.transform(output[\u001b[32m0\u001b[39m], y_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:259\u001b[39m, in \u001b[36mNearMiss._fit_resample\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    250\u001b[39m     index_target_class = \u001b[38;5;28mself\u001b[39m._selection_dist_based(\n\u001b[32m    251\u001b[39m         X,\n\u001b[32m    252\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m         sel_strategy=\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.version == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     dist_vec, idx_vec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_minority\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     index_target_class = \u001b[38;5;28mself\u001b[39m._selection_dist_based(\n\u001b[32m    263\u001b[39m         X,\n\u001b[32m    264\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m         sel_strategy=\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.version == \u001b[32m3\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:77\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:338\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.EuclideanArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:132\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikae\\toCode\\Artigo_IA\\ondocker\\venv\\Lib\\site-packages\\numpy\\_core\\numeric.py:352\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, device, like)\u001b[39m\n\u001b[32m    350\u001b[39m     fill_value = asarray(fill_value)\n\u001b[32m    351\u001b[39m     dtype = fill_value.dtype\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m a = \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m multiarray.copyto(a, fill_value, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 6.44 TiB for an array with shape (6191212, 142914) and data type int64"
     ]
    }
   ],
   "source": [
    "for fold_index, (train_index, test_index) in enumerate(kf.split(x1)):\n",
    "    X_train, X_test = x1[train_index], x1[test_index]\n",
    "    Y_train, Y_test = y1[train_index], y1[test_index]\n",
    "        \n",
    "    #isso aqui é o nearmiss e salvando os dados descartados concatenando no conjunto de testes\n",
    "    counts = Counter(Y_train)\n",
    "    valid_classes = {cls: 5000 for cls in range(5) if counts.get(cls, 0) >= 5000}\n",
    "\n",
    "    nm = NearMiss(sampling_strategy=valid_classes, version=2, n_neighbors=3, n_jobs=-1)\n",
    "    X_resampled, Y_resampled = nm.fit_resample(X_train, Y_train)\n",
    "\n",
    "    hash_train = hash_pandas_object(pd.DataFrame(X_train)).values\n",
    "    hash_resampled = hash_pandas_object(pd.DataFrame(X_resampled)).values\n",
    "    selected_mask = np.isin(hash_train, hash_resampled)\n",
    "\n",
    "    X_discarded = X_train[~selected_mask]\n",
    "    Y_discarded = Y_train[~selected_mask]\n",
    "\n",
    "    X_test = np.concatenate((X_test, X_discarded), axis=0)\n",
    "    Y_test = np.concatenate((Y_test, Y_discarded), axis=0)\n",
    "\n",
    "    X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_train, Y_train, random_state=42, test_size=0.2)\n",
    "\n",
    "    #normalizaçao dos dados\n",
    "    X_train_val = scaler.fit_transform(X_train_val) \n",
    "    X_test_val = scaler.transform(X_test_val)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    os.makedirs(f\"fold_{fold_index}\", exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(X_train_val).to_parquet(f\"fold_{fold_index}/X_train_val_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(X_test_val).to_parquet(f\"fold_{fold_index}/X_test_val_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(y_train_val).to_parquet(f\"fold_{fold_index}/Y_train_val_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(y_test_val).to_parquet(f\"fold_{fold_index}/Y_test_val_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(X_train).to_parquet(f\"fold_{fold_index}/X_train_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(X_test).to_parquet(f\"fold_{fold_index}/X_test_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(Y_train).to_parquet(f\"fold_{fold_index}/Y_train_fold{fold_index}.parquet\", index=False)\n",
    "    pd.DataFrame(Y_test).to_parquet(f\"fold_{fold_index}/Y_test_fold{fold_index}.parquet\", index=False)\n",
    "    \n",
    "    del X_train, X_test, Y_train, Y_test, X_train_val, X_test_val, y_train_val, y_test_val, X_resampled, Y_resampled, X_discarded, Y_discarded, hash_train, hash_resampled, selected_mask\n",
    "    gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
